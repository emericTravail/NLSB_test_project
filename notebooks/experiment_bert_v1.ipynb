{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f0e921ee",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f0e921ee",
        "outputId": "c289f3e1-2caf-42e6-b94e-70e9187d992d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  6 15:21:25 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModel, BertTokenizerFast, BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
        "\n"
      ],
      "metadata": {
        "id": "n5PofM31qQwT"
      },
      "id": "n5PofM31qQwT",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Working dir:', os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2w0XmPXtMnd",
        "outputId": "f7a34949-b525-40d9-fbe6-44232ea8d6b0"
      },
      "id": "n2w0XmPXtMnd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working dir: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "KQoRCYSqrEj4"
      },
      "id": "KQoRCYSqrEj4"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module to transform data to be consumable by model\n",
        "\"\"\"\n",
        "\n",
        "class DatabaseToBertDataset():\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) # (\"bert-base-multilingual-cased\")\n",
        "        self.tokenizerChunkLen = self.tokenizer.model_max_length\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        self.maxTokensLength = config.max_position_embeddings\n",
        "\n",
        "        # \"allenai/longformer-base-4096\" has no max_length\n",
        "        if model_name == \"abazoge/DrLongformer\":\n",
        "            self.maxTokensLength = 4096\n",
        "\n",
        "    def _tokenize(self, df: str) -> pd.DataFrame:\n",
        "        \"\"\"Tokenizer function\"\"\"\n",
        "        tokenized = self.tokenizer(\n",
        "            df[\"email\"].to_list(),\n",
        "            df[\"bill\"].to_list(),\n",
        "            padding='max_length',\n",
        "            max_length=self.maxTokensLength,\n",
        "            truncation=True,\n",
        "            stride=128,                        # overlap between chunks\n",
        "            return_overflowing_tokens=True,    # keep extra chunks\n",
        "            return_offsets_mapping=True,       # optional: track positions in original text\n",
        "            return_tensors=\"pt\"                # PyTorch tensors\n",
        "        )\n",
        "\n",
        "        # input(tokenized.keys())\n",
        "\n",
        "        # print(type(tokenized))\n",
        "        # print(tokenized['input_ids'].shape)\n",
        "        # print(tokenized['token_type_ids'].shape)\n",
        "        # print(tokenized['attention_mask'].shape)\n",
        "        # print(tokenized['offset_mapping'].shape)\n",
        "        # print(tokenized['overflow_to_sample_mapping'].shape)\n",
        "        # input('MMMM')\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def _encode_labels(self, df: pd.DataFrame) -> tuple[pd.DataFrame, np.ndarray]:\n",
        "        \"\"\"Encode labels\"\"\"\n",
        "\n",
        "        df = df.copy()\n",
        "        # Get unique labels from labels description\n",
        "        with open('labels.json', 'r') as file:\n",
        "            labels = json.load(file)['etiquettes']\n",
        "        labelIds = [label['id'] for label in labels]\n",
        "\n",
        "        # Format original labels\n",
        "        df['labels'] = df['labels'].apply(\n",
        "            lambda x: x.split('|') if isinstance(x, str) else []\n",
        "        )\n",
        "\n",
        "        # Encode\n",
        "        mlb = MultiLabelBinarizer(classes=labelIds)\n",
        "        encoded = mlb.fit_transform(df['labels'])\n",
        "        df = pd.concat(\n",
        "            [df, pd.DataFrame(encoded, columns=mlb.classes_)],\n",
        "            axis=1\n",
        "        )\n",
        "        df = df.drop(columns='labels')\n",
        "\n",
        "        return df, mlb.classes_\n",
        "\n",
        "\n",
        "    def execute(self, df : pd.DataFrame) -> tuple[dict[torch.Tensor], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Transform from database to dataset consumable by model\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize ('input_ids' 'attention_mask''token_type_ids' 'overflow_to_sample_mapping')\n",
        "        data = self._tokenize(df)\n",
        "\n",
        "        labelCols = None\n",
        "        if 'labels' in df.columns:\n",
        "            df, labelCols = self._encode_labels(df)\n",
        "\n",
        "            labels_tensor = torch.tensor(df[labelCols].to_numpy())\n",
        "            expanded_labels = labels_tensor[data['overflow_to_sample_mapping']]\n",
        "\n",
        "            data['labels'] = expanded_labels\n",
        "\n",
        "        return data, labelCols"
      ],
      "metadata": {
        "id": "I8x9eo3ZrNhv"
      },
      "id": "I8x9eo3ZrNhv",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "mNJOTyGHqJQg"
      },
      "id": "mNJOTyGHqJQg"
    },
    {
      "cell_type": "code",
      "source": [
        "BERT = \"bert-base-multilingual-cased\"\n",
        "LONGFORMER = \"abazoge/DrLongformer\"\n",
        "BIGBIRD = \"google/bigbird-roberta-base\"\n",
        "\n",
        "class BertMeanClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.modelName = BERT\n",
        "        self.bert = AutoModel.from_pretrained(self.modelName)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, batch, device):\n",
        "        input_ids, attention_mask, token_type_ids = [x.to(device) for x in batch]\n",
        "\n",
        "        # BERT outputs all hidden states\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        # The BERT model returns:\n",
        "        # - last_hidden_state: (batch_size, seq_len, hidden_dim)\n",
        "        # - pooler_output: (batch_size, hidden_dim)\n",
        "        last_hidden_state = outputs.last_hidden_state # (batch_size, seq_len, hidden_dim)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] embedding after tanh layer\n",
        "\n",
        "        # Apply dropout + classification on the pooled [CLS] representation\n",
        "        out = self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class LongformerClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        # \"allenai/longformer-base-4096\" NOT IN FRENCH... :(\n",
        "        self.modelName = LONGFORMER\n",
        "        self.bert = AutoModel.from_pretrained(self.modelName)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, batch, device):\n",
        "        input_ids, attention_mask = [x.to(device) for x in batch]\n",
        "\n",
        "        # BERT outputs all hidden states\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # The BERT model returns:\n",
        "        # - last_hidden_state: (batch_size, seq_len, hidden_dim)\n",
        "        # - pooler_output: (batch_size, hidden_dim)\n",
        "        last_hidden_state = outputs.last_hidden_state # (batch_size, seq_len, hidden_dim)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] embedding after tanh layer\n",
        "\n",
        "        # Apply dropout + classification on the pooled [CLS] representation\n",
        "        out = self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "        return out\n",
        "\n",
        "class BigbirdClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, freeze_bert=False):\n",
        "        super().__init__()\n",
        "        self.modelName = BIGBIRD\n",
        "        self.bert = AutoModel.from_pretrained(self.modelName)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, batch, device):\n",
        "        input_ids, attention_mask = [x.to(device) for x in batch]\n",
        "\n",
        "        # BERT outputs all hidden states\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # The BERT model returns:\n",
        "        # - last_hidden_state: (batch_size, seq_len, hidden_dim)\n",
        "        # - pooler_output: (batch_size, hidden_dim)\n",
        "        last_hidden_state = outputs.last_hidden_state # (batch_size, seq_len, hidden_dim)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] embedding after tanh layer\n",
        "\n",
        "        # Apply dropout + classification on the pooled [CLS] representation\n",
        "        out = self.classifier(self.dropout(pooled_output))\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "YCm7SwBiqODZ"
      },
      "id": "YCm7SwBiqODZ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "l60O-zfuRkTS"
      },
      "id": "l60O-zfuRkTS"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_eval_metrics() -> dict[list[float|dict]]:\n",
        "    \"\"\"\n",
        "    Init of the evaluation metrics\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'loss': [],\n",
        "        'accuracy': [],\n",
        "        'precision': [],\n",
        "        'recall': [],\n",
        "        'f1_micro': [],\n",
        "        'f1_macro_avg': [],\n",
        "        'f1_weighted_avg': [],\n",
        "        'f1_macro': []\n",
        "    }\n",
        "\n",
        "def compute_eval_metrics(loss, all_labels, all_preds, labels_list) -> dict[list[float]]:\n",
        "  \"\"\"\n",
        "  Computes evaluation metrics\n",
        "  \"\"\"\n",
        "  f1Report = classification_report(all_labels, all_preds, target_names=labels_list, output_dict=True, zero_division=0)\n",
        "\n",
        "\n",
        "  evalMetric = init_eval_metrics()\n",
        "  evalMetric['loss'].append(loss)\n",
        "  evalMetric['accuracy'].append(accuracy_score(all_labels, all_preds))\n",
        "  evalMetric['precision'].append(f1Report[\"micro avg\"][\"precision\"])\n",
        "  evalMetric['recall'].append(f1Report[\"micro avg\"][\"recall\"])\n",
        "  evalMetric['f1_micro'].append(f1Report[\"micro avg\"][\"f1-score\"])\n",
        "  evalMetric['f1_macro_avg'].append(f1Report[\"macro avg\"][\"f1-score\"])\n",
        "  evalMetric['f1_weighted_avg'].append(f1Report[\"weighted avg\"][\"f1-score\"])\n",
        "  evalMetric['f1_macro'].append(f1Report)\n",
        "\n",
        "  return evalMetric\n",
        "\n",
        "\n",
        "def save_eval_metrics(eval_metrics, evalMetric):\n",
        "    \"\"\"\n",
        "    Saves metrics\n",
        "    \"\"\"\n",
        "    for metricName, val in eval_metrics.items():\n",
        "        eval_metrics[metricName] = eval_metrics[metricName] + evalMetric[metricName]\n",
        "\n",
        "def print_eval_metrics(eval_metrics):\n",
        "    \"\"\"\n",
        "    Prints mean metrics\n",
        "    \"\"\"\n",
        "    loss = sum(eval_metrics['loss'])/len(eval_metrics['loss'])\n",
        "    accuracy = sum(eval_metrics['accuracy'])/len(eval_metrics['accuracy'])\n",
        "    precision = sum(eval_metrics['precision'])/len(eval_metrics['precision'])\n",
        "    recall = sum(eval_metrics['recall'])/len(eval_metrics['recall'])\n",
        "    f1Micro = sum(eval_metrics['f1_micro'])/len(eval_metrics['f1_micro'])\n",
        "    f1MacroAvg = sum(eval_metrics['f1_macro_avg'])/len(eval_metrics['f1_macro_avg'])\n",
        "\n",
        "\n",
        "    print(f\"Loss: {loss:.4f} F1-Micro: {f1Micro:.4f} F1-Macro-Avg {f1MacroAvg:.4f} Precision: {precision:.4f} Recall: {recall:.4f}\")\n",
        "\n",
        "\n",
        "def eval_metrics_to_df(eval_metrics) -> tuple[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Full display all metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Golbal metrics\n",
        "    globalMetrics = {}\n",
        "    for metricName, val in eval_metrics.items():\n",
        "        if 'f1_macro' == metricName:\n",
        "          continue\n",
        "        valMean = sum(eval_metrics[metricName])/len(eval_metrics[metricName])\n",
        "        globalMetrics[metricName] = valMean\n",
        "    globalMetricsDf = pd.DataFrame([globalMetrics])\n",
        "\n",
        "\n",
        "    # Label metrics\n",
        "    f1MacrosDfs = [pd.DataFrame(r).transpose() for r in eval_metrics['f1_macro']]\n",
        "    f1MacrosDf = pd.concat(f1MacrosDfs).groupby(level=0).mean(numeric_only=True)\n",
        "    f1MacrosDf = f1MacrosDf.drop(index=[\"micro avg\", \"macro avg\", \"weighted avg\", \"samples avg\"])\n",
        "\n",
        "    return globalMetricsDf, f1MacrosDf\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X01IDC7URjCX"
      },
      "id": "X01IDC7URjCX",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "TmoKNwW-rqH4"
      },
      "id": "TmoKNwW-rqH4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "df = pd.read_csv(os.path.join('data', 'parsed_synthetic', 'synth_data.csv'))\n",
        "trainValDf, testDf = train_test_split(df, test_size=0.05, random_state=42)\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_NAME =  LONGFORMER # BERT\n",
        "epochs = 20\n",
        "nKf = 5\n",
        "batch_size = 2\n",
        "threshold = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'CONFIG: batch_size={batch_size}, epochs={epochs}, device={device}')\n",
        "\n",
        "\n",
        "# === Eval Variables ===\n",
        "evalMetrics = init_eval_metrics()\n",
        "\n",
        "# === Cross Validation ===\n",
        "kf = KFold(n_splits=nKf, shuffle=True, random_state=42)\n",
        "crossVal = kf.split(trainValDf)\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f\"\\n===== Fold {fold+1} =====\")\n",
        "    trainDf = df.iloc[train_idx].copy().reset_index()\n",
        "    valDf   = df.iloc[val_idx].copy().reset_index()\n",
        "\n",
        "    # === Tokenize ===\n",
        "    databaseToBertDataset = DatabaseToBertDataset(model_name=MODEL_NAME)\n",
        "    train_data, labels_list = databaseToBertDataset.execute(trainDf)\n",
        "    val_data, _ = databaseToBertDataset.execute(valDf)\n",
        "\n",
        "    if MODEL_NAME == BERT:\n",
        "        # === Datasets ===\n",
        "        train_dataset = TensorDataset(\n",
        "            train_data[\"input_ids\"],\n",
        "            train_data[\"attention_mask\"],\n",
        "            train_data[\"token_type_ids\"],\n",
        "            train_data[\"labels\"].float()\n",
        "        )\n",
        "        val_dataset = TensorDataset(\n",
        "            val_data[\"input_ids\"],\n",
        "            val_data[\"attention_mask\"],\n",
        "            val_data[\"token_type_ids\"],\n",
        "            val_data[\"labels\"].float()\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # === Model ===\n",
        "        model = BertMeanClassifier(MODEL_NAME, num_labels=len(labels_list))\n",
        "\n",
        "\n",
        "    elif MODEL_NAME == LONGFORMER:\n",
        "\n",
        "        # === Datasets ===\n",
        "        train_dataset = TensorDataset(\n",
        "            train_data[\"input_ids\"],\n",
        "            train_data[\"attention_mask\"],\n",
        "            train_data[\"labels\"].float()\n",
        "        )\n",
        "        val_dataset = TensorDataset(\n",
        "            val_data[\"input_ids\"],\n",
        "            val_data[\"attention_mask\"],\n",
        "            val_data[\"labels\"].float()\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # === Model ===\n",
        "        model = LongformerClassifier(MODEL_NAME, num_labels=len(labels_list))\n",
        "\n",
        "    elif MODEL_NAME == BIGBIRD:\n",
        "\n",
        "        # === Datasets ===\n",
        "        train_dataset = TensorDataset(\n",
        "            train_data[\"input_ids\"],\n",
        "            train_data[\"attention_mask\"],\n",
        "            train_data[\"labels\"].float()\n",
        "        )\n",
        "        val_dataset = TensorDataset(\n",
        "            val_data[\"input_ids\"],\n",
        "            val_data[\"attention_mask\"],\n",
        "            val_data[\"labels\"].float()\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # === Model ===\n",
        "        model = BigbirdClassifier(MODEL_NAME, num_labels=len(labels_list))\n",
        "\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # === TRAINING LOOP ===\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n===== EPOCH {epoch+1}/{epochs} =====\")\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            labels = batch[-1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(batch[:-1], device)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 10 == 0 or batch_idx == 0:\n",
        "                print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # === EVALUATION LOOP ===\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                labels = batch[-1].to(device)\n",
        "                outputs = model(batch[:-1], device)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                eval_loss += loss.item()\n",
        "\n",
        "                # --- Predictions ---\n",
        "                preds = torch.sigmoid(outputs) > threshold  # (batch_size, num_labels)\n",
        "\n",
        "                # Move to CPU for sklearn\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "        # === Combine batches ===\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "        # === Metrics ===\n",
        "        avg_eval_loss = eval_loss / len(val_loader)\n",
        "        evalMetric = compute_eval_metrics(avg_eval_loss, all_labels, all_preds, labels_list)\n",
        "        print_eval_metrics(evalMetric)\n",
        "\n",
        "    save_eval_metrics(evalMetrics, evalMetric)\n",
        "\n",
        "\n",
        "print_eval_metrics(evalMetrics)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyhHeGf_rtEI",
        "outputId": "318fd6ac-5b81-4ae3-8c80-7e3c60acb39c"
      },
      "id": "YyhHeGf_rtEI",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG: batch_size=2, epochs=20, device=cuda\n",
            "\n",
            "===== Fold 1 =====\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "\n",
            "===== EPOCH 1/20 =====\n",
            "  Batch 1/61 - Loss: 0.8321\n",
            "  Batch 10/61 - Loss: 0.6434\n",
            "  Batch 20/61 - Loss: 0.6763\n",
            "  Batch 30/61 - Loss: 0.5856\n",
            "  Batch 40/61 - Loss: 0.4937\n",
            "  Batch 50/61 - Loss: 0.5267\n",
            "  Batch 60/61 - Loss: 0.4717\n",
            "Loss: 0.5284 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 2/20 =====\n",
            "  Batch 1/61 - Loss: 0.4755\n",
            "  Batch 10/61 - Loss: 0.6179\n",
            "  Batch 20/61 - Loss: 0.4472\n",
            "  Batch 30/61 - Loss: 0.4903\n",
            "  Batch 40/61 - Loss: 0.5575\n",
            "  Batch 50/61 - Loss: 0.3684\n",
            "  Batch 60/61 - Loss: 0.5855\n",
            "Loss: 0.5288 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 3/20 =====\n",
            "  Batch 1/61 - Loss: 0.5898\n",
            "  Batch 10/61 - Loss: 0.5337\n",
            "  Batch 20/61 - Loss: 0.7546\n",
            "  Batch 30/61 - Loss: 0.5000\n",
            "  Batch 40/61 - Loss: 0.5496\n",
            "  Batch 50/61 - Loss: 0.5138\n",
            "  Batch 60/61 - Loss: 0.4625\n",
            "Loss: 0.5287 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 4/20 =====\n",
            "  Batch 1/61 - Loss: 0.5981\n",
            "  Batch 10/61 - Loss: 0.4322\n",
            "  Batch 20/61 - Loss: 0.4263\n",
            "  Batch 30/61 - Loss: 0.5367\n",
            "  Batch 40/61 - Loss: 0.7258\n",
            "  Batch 50/61 - Loss: 0.5285\n",
            "  Batch 60/61 - Loss: 0.6236\n",
            "Loss: 0.5298 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 5/20 =====\n",
            "  Batch 1/61 - Loss: 0.4279\n",
            "  Batch 10/61 - Loss: 0.3415\n",
            "  Batch 20/61 - Loss: 0.5148\n",
            "  Batch 30/61 - Loss: 0.4927\n",
            "  Batch 40/61 - Loss: 0.5151\n",
            "  Batch 50/61 - Loss: 0.5448\n",
            "  Batch 60/61 - Loss: 0.6114\n",
            "Loss: 0.5298 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 6/20 =====\n",
            "  Batch 1/61 - Loss: 0.4973\n",
            "  Batch 10/61 - Loss: 0.5745\n",
            "  Batch 20/61 - Loss: 0.4591\n",
            "  Batch 30/61 - Loss: 0.5123\n",
            "  Batch 40/61 - Loss: 0.4010\n",
            "  Batch 50/61 - Loss: 0.5262\n",
            "  Batch 60/61 - Loss: 0.3877\n",
            "Loss: 0.5140 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 7/20 =====\n",
            "  Batch 1/61 - Loss: 0.2326\n",
            "  Batch 10/61 - Loss: 0.3085\n",
            "  Batch 20/61 - Loss: 0.3937\n",
            "  Batch 30/61 - Loss: 0.3441\n",
            "  Batch 40/61 - Loss: 0.5527\n",
            "  Batch 50/61 - Loss: 0.5467\n",
            "  Batch 60/61 - Loss: 0.4355\n",
            "Loss: 0.4861 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 8/20 =====\n",
            "  Batch 1/61 - Loss: 0.6693\n",
            "  Batch 10/61 - Loss: 0.6356\n",
            "  Batch 20/61 - Loss: 0.3136\n",
            "  Batch 30/61 - Loss: 0.3431\n",
            "  Batch 40/61 - Loss: 0.5249\n",
            "  Batch 50/61 - Loss: 0.4133\n",
            "  Batch 60/61 - Loss: 0.6280\n",
            "Loss: 0.5108 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 9/20 =====\n",
            "  Batch 1/61 - Loss: 0.6771\n",
            "  Batch 10/61 - Loss: 0.5961\n",
            "  Batch 20/61 - Loss: 0.2926\n",
            "  Batch 30/61 - Loss: 0.5352\n",
            "  Batch 40/61 - Loss: 0.2559\n",
            "  Batch 50/61 - Loss: 0.4255\n",
            "  Batch 60/61 - Loss: 0.4043\n",
            "Loss: 0.5287 F1-Micro: 0.0286 F1-Macro-Avg 0.0165 Precision: 0.3333 Recall: 0.0149\n",
            "\n",
            "===== EPOCH 10/20 =====\n",
            "  Batch 1/61 - Loss: 0.4946\n",
            "  Batch 10/61 - Loss: 0.5549\n",
            "  Batch 20/61 - Loss: 0.5681\n",
            "  Batch 30/61 - Loss: 0.4508\n",
            "  Batch 40/61 - Loss: 0.4074\n",
            "  Batch 50/61 - Loss: 0.5385\n",
            "  Batch 60/61 - Loss: 0.3293\n",
            "Loss: 0.4953 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 11/20 =====\n",
            "  Batch 1/61 - Loss: 0.5328\n",
            "  Batch 10/61 - Loss: 0.3978\n",
            "  Batch 20/61 - Loss: 0.4806\n",
            "  Batch 30/61 - Loss: 0.3330\n",
            "  Batch 40/61 - Loss: 0.4770\n",
            "  Batch 50/61 - Loss: 0.7408\n",
            "  Batch 60/61 - Loss: 0.4572\n",
            "Loss: 0.5062 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 12/20 =====\n",
            "  Batch 1/61 - Loss: 0.3859\n",
            "  Batch 10/61 - Loss: 0.6043\n",
            "  Batch 20/61 - Loss: 0.4810\n",
            "  Batch 30/61 - Loss: 0.3833\n",
            "  Batch 40/61 - Loss: 0.3985\n",
            "  Batch 50/61 - Loss: 0.3221\n",
            "  Batch 60/61 - Loss: 0.2778\n",
            "Loss: 0.5056 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 13/20 =====\n",
            "  Batch 1/61 - Loss: 0.2393\n",
            "  Batch 10/61 - Loss: 0.5233\n",
            "  Batch 20/61 - Loss: 0.6407\n",
            "  Batch 30/61 - Loss: 0.3743\n",
            "  Batch 40/61 - Loss: 0.4325\n",
            "  Batch 50/61 - Loss: 0.6715\n",
            "  Batch 60/61 - Loss: 0.5503\n",
            "Loss: 0.5138 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 14/20 =====\n",
            "  Batch 1/61 - Loss: 0.2553\n",
            "  Batch 10/61 - Loss: 0.4432\n",
            "  Batch 20/61 - Loss: 0.3812\n",
            "  Batch 30/61 - Loss: 0.4460\n",
            "  Batch 40/61 - Loss: 0.3178\n",
            "  Batch 50/61 - Loss: 0.4575\n",
            "  Batch 60/61 - Loss: 0.1966\n",
            "Loss: 0.5011 F1-Micro: 0.0548 F1-Macro-Avg 0.0260 Precision: 0.3333 Recall: 0.0299\n",
            "\n",
            "===== EPOCH 15/20 =====\n",
            "  Batch 1/61 - Loss: 0.4171\n",
            "  Batch 10/61 - Loss: 0.4281\n",
            "  Batch 20/61 - Loss: 0.3603\n",
            "  Batch 30/61 - Loss: 0.6201\n",
            "  Batch 40/61 - Loss: 0.3019\n",
            "  Batch 50/61 - Loss: 0.3661\n",
            "  Batch 60/61 - Loss: 0.5522\n",
            "Loss: 0.5005 F1-Micro: 0.1928 F1-Macro-Avg 0.0947 Precision: 0.5000 Recall: 0.1194\n",
            "\n",
            "===== EPOCH 16/20 =====\n",
            "  Batch 1/61 - Loss: 0.3511\n",
            "  Batch 10/61 - Loss: 0.5348\n",
            "  Batch 20/61 - Loss: 0.2901\n",
            "  Batch 30/61 - Loss: 0.5058\n",
            "  Batch 40/61 - Loss: 0.4125\n",
            "  Batch 50/61 - Loss: 0.4706\n",
            "  Batch 60/61 - Loss: 0.6055\n",
            "Loss: 0.4902 F1-Micro: 0.1928 F1-Macro-Avg 0.0925 Precision: 0.5000 Recall: 0.1194\n",
            "\n",
            "===== EPOCH 17/20 =====\n",
            "  Batch 1/61 - Loss: 0.3117\n",
            "  Batch 10/61 - Loss: 0.3651\n",
            "  Batch 20/61 - Loss: 0.5066\n",
            "  Batch 30/61 - Loss: 0.4065\n",
            "  Batch 40/61 - Loss: 0.3440\n",
            "  Batch 50/61 - Loss: 0.4898\n",
            "  Batch 60/61 - Loss: 0.3199\n",
            "Loss: 0.4858 F1-Micro: 0.1299 F1-Macro-Avg 0.1090 Precision: 0.5000 Recall: 0.0746\n",
            "\n",
            "===== EPOCH 18/20 =====\n",
            "  Batch 1/61 - Loss: 0.2458\n",
            "  Batch 10/61 - Loss: 0.3548\n",
            "  Batch 20/61 - Loss: 0.4971\n",
            "  Batch 30/61 - Loss: 0.3024\n",
            "  Batch 40/61 - Loss: 0.4542\n",
            "  Batch 50/61 - Loss: 0.4376\n",
            "  Batch 60/61 - Loss: 0.4741\n",
            "Loss: 0.4823 F1-Micro: 0.1772 F1-Macro-Avg 0.1270 Precision: 0.5833 Recall: 0.1045\n",
            "\n",
            "===== EPOCH 19/20 =====\n",
            "  Batch 1/61 - Loss: 0.3409\n",
            "  Batch 10/61 - Loss: 0.2443\n",
            "  Batch 20/61 - Loss: 0.1721\n",
            "  Batch 30/61 - Loss: 0.4241\n",
            "  Batch 40/61 - Loss: 0.5033\n",
            "  Batch 50/61 - Loss: 0.3690\n",
            "  Batch 60/61 - Loss: 0.2823\n",
            "Loss: 0.4805 F1-Micro: 0.2410 F1-Macro-Avg 0.1132 Precision: 0.6250 Recall: 0.1493\n",
            "\n",
            "===== EPOCH 20/20 =====\n",
            "  Batch 1/61 - Loss: 0.3875\n",
            "  Batch 10/61 - Loss: 0.4227\n",
            "  Batch 20/61 - Loss: 0.5075\n",
            "  Batch 30/61 - Loss: 0.3165\n",
            "  Batch 40/61 - Loss: 0.5254\n",
            "  Batch 50/61 - Loss: 0.3833\n",
            "  Batch 60/61 - Loss: 0.3512\n",
            "Loss: 0.4642 F1-Micro: 0.2195 F1-Macro-Avg 0.1000 Precision: 0.6000 Recall: 0.1343\n",
            "\n",
            "===== Fold 2 =====\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "\n",
            "===== EPOCH 1/20 =====\n",
            "  Batch 1/60 - Loss: 0.6950\n",
            "  Batch 10/60 - Loss: 0.6870\n",
            "  Batch 20/60 - Loss: 0.6640\n",
            "  Batch 30/60 - Loss: 0.5632\n",
            "  Batch 40/60 - Loss: 0.6288\n",
            "  Batch 50/60 - Loss: 0.4925\n",
            "  Batch 60/60 - Loss: 0.4570\n",
            "Loss: 0.5231 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 2/20 =====\n",
            "  Batch 1/60 - Loss: 0.3761\n",
            "  Batch 10/60 - Loss: 0.4518\n",
            "  Batch 20/60 - Loss: 0.4805\n",
            "  Batch 30/60 - Loss: 0.5506\n",
            "  Batch 40/60 - Loss: 0.5914\n",
            "  Batch 50/60 - Loss: 0.5938\n",
            "  Batch 60/60 - Loss: 0.4983\n",
            "Loss: 0.5167 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 3/20 =====\n",
            "  Batch 1/60 - Loss: 0.4169\n",
            "  Batch 10/60 - Loss: 0.5837\n",
            "  Batch 20/60 - Loss: 0.5742\n",
            "  Batch 30/60 - Loss: 0.4755\n",
            "  Batch 40/60 - Loss: 0.5536\n",
            "  Batch 50/60 - Loss: 0.3364\n",
            "  Batch 60/60 - Loss: 0.4980\n",
            "Loss: 0.5167 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 4/20 =====\n",
            "  Batch 1/60 - Loss: 0.4534\n",
            "  Batch 10/60 - Loss: 0.5367\n",
            "  Batch 20/60 - Loss: 0.3960\n",
            "  Batch 30/60 - Loss: 0.6282\n",
            "  Batch 40/60 - Loss: 0.5350\n",
            "  Batch 50/60 - Loss: 0.3508\n",
            "  Batch 60/60 - Loss: 0.3362\n",
            "Loss: 0.5130 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 5/20 =====\n",
            "  Batch 1/60 - Loss: 0.3816\n",
            "  Batch 10/60 - Loss: 0.3700\n",
            "  Batch 20/60 - Loss: 0.5913\n",
            "  Batch 30/60 - Loss: 0.4621\n",
            "  Batch 40/60 - Loss: 0.5329\n",
            "  Batch 50/60 - Loss: 0.5787\n",
            "  Batch 60/60 - Loss: 0.5239\n",
            "Loss: 0.5151 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 6/20 =====\n",
            "  Batch 1/60 - Loss: 0.5319\n",
            "  Batch 10/60 - Loss: 0.4272\n",
            "  Batch 20/60 - Loss: 0.5321\n",
            "  Batch 30/60 - Loss: 0.4878\n",
            "  Batch 40/60 - Loss: 0.4426\n",
            "  Batch 50/60 - Loss: 0.5161\n",
            "  Batch 60/60 - Loss: 0.4894\n",
            "Loss: 0.5098 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 7/20 =====\n",
            "  Batch 1/60 - Loss: 0.4667\n",
            "  Batch 10/60 - Loss: 0.5010\n",
            "  Batch 20/60 - Loss: 0.3338\n",
            "  Batch 30/60 - Loss: 0.6402\n",
            "  Batch 40/60 - Loss: 0.4674\n",
            "  Batch 50/60 - Loss: 0.4557\n",
            "  Batch 60/60 - Loss: 0.4391\n",
            "Loss: 0.5210 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 8/20 =====\n",
            "  Batch 1/60 - Loss: 0.3832\n",
            "  Batch 10/60 - Loss: 0.5471\n",
            "  Batch 20/60 - Loss: 0.3786\n",
            "  Batch 30/60 - Loss: 0.3796\n",
            "  Batch 40/60 - Loss: 0.2248\n",
            "  Batch 50/60 - Loss: 0.2760\n",
            "  Batch 60/60 - Loss: 0.4466\n",
            "Loss: 0.5073 F1-Micro: 0.0533 F1-Macro-Avg 0.0455 Precision: 0.6667 Recall: 0.0278\n",
            "\n",
            "===== EPOCH 9/20 =====\n",
            "  Batch 1/60 - Loss: 0.6599\n",
            "  Batch 10/60 - Loss: 0.4332\n",
            "  Batch 20/60 - Loss: 0.3822\n",
            "  Batch 30/60 - Loss: 0.4623\n",
            "  Batch 40/60 - Loss: 0.3274\n",
            "  Batch 50/60 - Loss: 0.4741\n",
            "  Batch 60/60 - Loss: 0.4774\n",
            "Loss: 0.4986 F1-Micro: 0.0533 F1-Macro-Avg 0.0455 Precision: 0.6667 Recall: 0.0278\n",
            "\n",
            "===== EPOCH 10/20 =====\n",
            "  Batch 1/60 - Loss: 0.5170\n",
            "  Batch 10/60 - Loss: 0.3758\n",
            "  Batch 20/60 - Loss: 0.4578\n",
            "  Batch 30/60 - Loss: 0.4357\n",
            "  Batch 40/60 - Loss: 0.4317\n",
            "  Batch 50/60 - Loss: 0.5500\n",
            "  Batch 60/60 - Loss: 0.2854\n",
            "Loss: 0.4908 F1-Micro: 0.0533 F1-Macro-Avg 0.0455 Precision: 0.6667 Recall: 0.0278\n",
            "\n",
            "===== EPOCH 11/20 =====\n",
            "  Batch 1/60 - Loss: 0.2644\n",
            "  Batch 10/60 - Loss: 0.5751\n",
            "  Batch 20/60 - Loss: 0.5308\n",
            "  Batch 30/60 - Loss: 0.5843\n",
            "  Batch 40/60 - Loss: 0.4061\n",
            "  Batch 50/60 - Loss: 0.5574\n",
            "  Batch 60/60 - Loss: 0.3422\n",
            "Loss: 0.4837 F1-Micro: 0.1975 F1-Macro-Avg 0.1255 Precision: 0.8889 Recall: 0.1111\n",
            "\n",
            "===== EPOCH 12/20 =====\n",
            "  Batch 1/60 - Loss: 0.4381\n",
            "  Batch 10/60 - Loss: 0.3768\n",
            "  Batch 20/60 - Loss: 0.4379\n",
            "  Batch 30/60 - Loss: 0.4583\n",
            "  Batch 40/60 - Loss: 0.4390\n",
            "  Batch 50/60 - Loss: 0.3514\n",
            "  Batch 60/60 - Loss: 0.3357\n",
            "Loss: 0.4822 F1-Micro: 0.1750 F1-Macro-Avg 0.1104 Precision: 0.8750 Recall: 0.0972\n",
            "\n",
            "===== EPOCH 13/20 =====\n",
            "  Batch 1/60 - Loss: 0.3259\n",
            "  Batch 10/60 - Loss: 0.6140\n",
            "  Batch 20/60 - Loss: 0.4332\n",
            "  Batch 30/60 - Loss: 0.3168\n",
            "  Batch 40/60 - Loss: 0.3679\n",
            "  Batch 50/60 - Loss: 0.3173\n",
            "  Batch 60/60 - Loss: 0.3677\n",
            "Loss: 0.4763 F1-Micro: 0.1975 F1-Macro-Avg 0.1182 Precision: 0.8889 Recall: 0.1111\n",
            "\n",
            "===== EPOCH 14/20 =====\n",
            "  Batch 1/60 - Loss: 0.2514\n",
            "  Batch 10/60 - Loss: 0.4233\n",
            "  Batch 20/60 - Loss: 0.5413\n",
            "  Batch 30/60 - Loss: 0.5411\n",
            "  Batch 40/60 - Loss: 0.5744\n",
            "  Batch 50/60 - Loss: 0.3905\n",
            "  Batch 60/60 - Loss: 0.3929\n",
            "Loss: 0.4712 F1-Micro: 0.3226 F1-Macro-Avg 0.2818 Precision: 0.7143 Recall: 0.2083\n",
            "\n",
            "===== EPOCH 15/20 =====\n",
            "  Batch 1/60 - Loss: 0.2345\n",
            "  Batch 10/60 - Loss: 0.2224\n",
            "  Batch 20/60 - Loss: 0.4466\n",
            "  Batch 30/60 - Loss: 0.5534\n",
            "  Batch 40/60 - Loss: 0.1992\n",
            "  Batch 50/60 - Loss: 0.4851\n",
            "  Batch 60/60 - Loss: 0.3751\n",
            "Loss: 0.4274 F1-Micro: 0.3146 F1-Macro-Avg 0.2319 Precision: 0.8235 Recall: 0.1944\n",
            "\n",
            "===== EPOCH 16/20 =====\n",
            "  Batch 1/60 - Loss: 0.1653\n",
            "  Batch 10/60 - Loss: 0.3020\n",
            "  Batch 20/60 - Loss: 0.3328\n",
            "  Batch 30/60 - Loss: 0.3146\n",
            "  Batch 40/60 - Loss: 0.3498\n",
            "  Batch 50/60 - Loss: 0.2823\n",
            "  Batch 60/60 - Loss: 0.2816\n",
            "Loss: 0.4147 F1-Micro: 0.4510 F1-Macro-Avg 0.4030 Precision: 0.7667 Recall: 0.3194\n",
            "\n",
            "===== EPOCH 17/20 =====\n",
            "  Batch 1/60 - Loss: 0.1917\n",
            "  Batch 10/60 - Loss: 0.3163\n",
            "  Batch 20/60 - Loss: 0.4082\n",
            "  Batch 30/60 - Loss: 0.4014\n",
            "  Batch 40/60 - Loss: 0.3375\n",
            "  Batch 50/60 - Loss: 0.4205\n",
            "  Batch 60/60 - Loss: 0.3842\n",
            "Loss: 0.4002 F1-Micro: 0.4167 F1-Macro-Avg 0.3493 Precision: 0.8333 Recall: 0.2778\n",
            "\n",
            "===== EPOCH 18/20 =====\n",
            "  Batch 1/60 - Loss: 0.4198\n",
            "  Batch 10/60 - Loss: 0.1710\n",
            "  Batch 20/60 - Loss: 0.2421\n",
            "  Batch 30/60 - Loss: 0.3241\n",
            "  Batch 40/60 - Loss: 0.1947\n",
            "  Batch 50/60 - Loss: 0.2108\n",
            "  Batch 60/60 - Loss: 0.2032\n",
            "Loss: 0.4171 F1-Micro: 0.5738 F1-Macro-Avg 0.4792 Precision: 0.7000 Recall: 0.4861\n",
            "\n",
            "===== EPOCH 19/20 =====\n",
            "  Batch 1/60 - Loss: 0.1811\n",
            "  Batch 10/60 - Loss: 0.2639\n",
            "  Batch 20/60 - Loss: 0.2613\n",
            "  Batch 30/60 - Loss: 0.4051\n",
            "  Batch 40/60 - Loss: 0.3341\n",
            "  Batch 50/60 - Loss: 0.2713\n",
            "  Batch 60/60 - Loss: 0.1801\n",
            "Loss: 0.3733 F1-Micro: 0.5664 F1-Macro-Avg 0.4869 Precision: 0.7805 Recall: 0.4444\n",
            "\n",
            "===== EPOCH 20/20 =====\n",
            "  Batch 1/60 - Loss: 0.0698\n",
            "  Batch 10/60 - Loss: 0.2930\n",
            "  Batch 20/60 - Loss: 0.2472\n",
            "  Batch 30/60 - Loss: 0.2884\n",
            "  Batch 40/60 - Loss: 0.2637\n",
            "  Batch 50/60 - Loss: 0.1744\n",
            "  Batch 60/60 - Loss: 0.2363\n",
            "Loss: 0.3772 F1-Micro: 0.5818 F1-Macro-Avg 0.4566 Precision: 0.8421 Recall: 0.4444\n",
            "\n",
            "===== Fold 3 =====\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "\n",
            "===== EPOCH 1/20 =====\n",
            "  Batch 1/61 - Loss: 0.7350\n",
            "  Batch 10/61 - Loss: 0.6284\n",
            "  Batch 20/61 - Loss: 0.5391\n",
            "  Batch 30/61 - Loss: 0.5758\n",
            "  Batch 40/61 - Loss: 0.5470\n",
            "  Batch 50/61 - Loss: 0.4515\n",
            "  Batch 60/61 - Loss: 0.4287\n",
            "Loss: 0.4906 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 2/20 =====\n",
            "  Batch 1/61 - Loss: 0.5649\n",
            "  Batch 10/61 - Loss: 0.4052\n",
            "  Batch 20/61 - Loss: 0.3792\n",
            "  Batch 30/61 - Loss: 0.4443\n",
            "  Batch 40/61 - Loss: 0.4327\n",
            "  Batch 50/61 - Loss: 0.6765\n",
            "  Batch 60/61 - Loss: 0.3940\n",
            "Loss: 0.4982 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 3/20 =====\n",
            "  Batch 1/61 - Loss: 0.5022\n",
            "  Batch 10/61 - Loss: 0.2751\n",
            "  Batch 20/61 - Loss: 0.8265\n",
            "  Batch 30/61 - Loss: 0.6047\n",
            "  Batch 40/61 - Loss: 0.5701\n",
            "  Batch 50/61 - Loss: 0.5764\n",
            "  Batch 60/61 - Loss: 0.3508\n",
            "Loss: 0.4894 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 4/20 =====\n",
            "  Batch 1/61 - Loss: 0.5904\n",
            "  Batch 10/61 - Loss: 0.5191\n",
            "  Batch 20/61 - Loss: 0.5626\n",
            "  Batch 30/61 - Loss: 0.2762\n",
            "  Batch 40/61 - Loss: 0.4427\n",
            "  Batch 50/61 - Loss: 0.4668\n",
            "  Batch 60/61 - Loss: 0.3850\n",
            "Loss: 0.4907 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 5/20 =====\n",
            "  Batch 1/61 - Loss: 0.6709\n",
            "  Batch 10/61 - Loss: 0.4768\n",
            "  Batch 20/61 - Loss: 0.4094\n",
            "  Batch 30/61 - Loss: 0.2437\n",
            "  Batch 40/61 - Loss: 0.3887\n",
            "  Batch 50/61 - Loss: 0.5557\n",
            "  Batch 60/61 - Loss: 0.4082\n",
            "Loss: 0.4858 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 6/20 =====\n",
            "  Batch 1/61 - Loss: 0.5047\n",
            "  Batch 10/61 - Loss: 0.4285\n",
            "  Batch 20/61 - Loss: 0.4185\n",
            "  Batch 30/61 - Loss: 0.4629\n",
            "  Batch 40/61 - Loss: 0.4332\n",
            "  Batch 50/61 - Loss: 0.2576\n",
            "  Batch 60/61 - Loss: 0.7575\n",
            "Loss: 0.4940 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 7/20 =====\n",
            "  Batch 1/61 - Loss: 0.4042\n",
            "  Batch 10/61 - Loss: 0.5155\n",
            "  Batch 20/61 - Loss: 0.4580\n",
            "  Batch 30/61 - Loss: 0.4536\n",
            "  Batch 40/61 - Loss: 0.3422\n",
            "  Batch 50/61 - Loss: 0.5682\n",
            "  Batch 60/61 - Loss: 0.4255\n",
            "Loss: 0.4976 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 8/20 =====\n",
            "  Batch 1/61 - Loss: 0.5103\n",
            "  Batch 10/61 - Loss: 0.5956\n",
            "  Batch 20/61 - Loss: 0.5538\n",
            "  Batch 30/61 - Loss: 0.5340\n",
            "  Batch 40/61 - Loss: 0.2792\n",
            "  Batch 50/61 - Loss: 0.5028\n",
            "  Batch 60/61 - Loss: 0.6778\n",
            "Loss: 0.4872 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 9/20 =====\n",
            "  Batch 1/61 - Loss: 0.4809\n",
            "  Batch 10/61 - Loss: 0.6376\n",
            "  Batch 20/61 - Loss: 0.3137\n",
            "  Batch 30/61 - Loss: 0.2761\n",
            "  Batch 40/61 - Loss: 0.5878\n",
            "  Batch 50/61 - Loss: 0.4006\n",
            "  Batch 60/61 - Loss: 0.6262\n",
            "Loss: 0.4880 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 10/20 =====\n",
            "  Batch 1/61 - Loss: 0.4923\n",
            "  Batch 10/61 - Loss: 0.4983\n",
            "  Batch 20/61 - Loss: 0.1782\n",
            "  Batch 30/61 - Loss: 0.4327\n",
            "  Batch 40/61 - Loss: 0.5910\n",
            "  Batch 50/61 - Loss: 0.6257\n",
            "  Batch 60/61 - Loss: 0.4200\n",
            "Loss: 0.4949 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 11/20 =====\n",
            "  Batch 1/61 - Loss: 0.3360\n",
            "  Batch 10/61 - Loss: 0.5769\n",
            "  Batch 20/61 - Loss: 0.6258\n",
            "  Batch 30/61 - Loss: 0.5636\n",
            "  Batch 40/61 - Loss: 0.4428\n",
            "  Batch 50/61 - Loss: 0.3320\n",
            "  Batch 60/61 - Loss: 0.8025\n",
            "Loss: 0.4955 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 12/20 =====\n",
            "  Batch 1/61 - Loss: 0.4732\n",
            "  Batch 10/61 - Loss: 0.4332\n",
            "  Batch 20/61 - Loss: 0.5787\n",
            "  Batch 30/61 - Loss: 0.8100\n",
            "  Batch 40/61 - Loss: 0.4835\n",
            "  Batch 50/61 - Loss: 0.6747\n",
            "  Batch 60/61 - Loss: 0.3790\n",
            "Loss: 0.4888 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 13/20 =====\n",
            "  Batch 1/61 - Loss: 0.5307\n",
            "  Batch 10/61 - Loss: 0.5230\n",
            "  Batch 20/61 - Loss: 0.3988\n",
            "  Batch 30/61 - Loss: 0.5241\n",
            "  Batch 40/61 - Loss: 0.5168\n",
            "  Batch 50/61 - Loss: 0.4503\n",
            "  Batch 60/61 - Loss: 0.5169\n",
            "Loss: 0.4949 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 14/20 =====\n",
            "  Batch 1/61 - Loss: 0.4921\n",
            "  Batch 10/61 - Loss: 0.3162\n",
            "  Batch 20/61 - Loss: 0.4490\n",
            "  Batch 30/61 - Loss: 0.4931\n",
            "  Batch 40/61 - Loss: 0.4833\n",
            "  Batch 50/61 - Loss: 0.4806\n",
            "  Batch 60/61 - Loss: 0.4993\n",
            "Loss: 0.4945 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 15/20 =====\n",
            "  Batch 1/61 - Loss: 0.2237\n",
            "  Batch 10/61 - Loss: 0.4463\n",
            "  Batch 20/61 - Loss: 0.4671\n",
            "  Batch 30/61 - Loss: 0.5043\n",
            "  Batch 40/61 - Loss: 0.5702\n",
            "  Batch 50/61 - Loss: 0.4176\n",
            "  Batch 60/61 - Loss: 0.5605\n",
            "Loss: 0.4944 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 16/20 =====\n",
            "  Batch 1/61 - Loss: 0.6639\n",
            "  Batch 10/61 - Loss: 0.4254\n",
            "  Batch 20/61 - Loss: 0.4632\n",
            "  Batch 30/61 - Loss: 0.4281\n",
            "  Batch 40/61 - Loss: 0.5501\n",
            "  Batch 50/61 - Loss: 0.3726\n",
            "  Batch 60/61 - Loss: 0.5159\n",
            "Loss: 0.4915 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 17/20 =====\n",
            "  Batch 1/61 - Loss: 0.3206\n",
            "  Batch 10/61 - Loss: 0.5288\n",
            "  Batch 20/61 - Loss: 0.5650\n",
            "  Batch 30/61 - Loss: 0.4945\n",
            "  Batch 40/61 - Loss: 0.4799\n",
            "  Batch 50/61 - Loss: 0.3999\n",
            "  Batch 60/61 - Loss: 0.4856\n",
            "Loss: 0.4878 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 18/20 =====\n",
            "  Batch 1/61 - Loss: 0.4250\n",
            "  Batch 10/61 - Loss: 0.3660\n",
            "  Batch 20/61 - Loss: 0.3937\n",
            "  Batch 30/61 - Loss: 0.4533\n",
            "  Batch 40/61 - Loss: 0.3517\n",
            "  Batch 50/61 - Loss: 0.5684\n",
            "  Batch 60/61 - Loss: 0.6414\n",
            "Loss: 0.4926 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 19/20 =====\n",
            "  Batch 1/61 - Loss: 0.4562\n",
            "  Batch 10/61 - Loss: 0.6932\n",
            "  Batch 20/61 - Loss: 0.4514\n",
            "  Batch 30/61 - Loss: 0.4648\n",
            "  Batch 40/61 - Loss: 0.6224\n",
            "  Batch 50/61 - Loss: 0.5545\n",
            "  Batch 60/61 - Loss: 0.5891\n",
            "Loss: 0.4882 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 20/20 =====\n",
            "  Batch 1/61 - Loss: 0.4080\n",
            "  Batch 10/61 - Loss: 0.6553\n",
            "  Batch 20/61 - Loss: 0.6041\n",
            "  Batch 30/61 - Loss: 0.5644\n",
            "  Batch 40/61 - Loss: 0.6677\n",
            "  Batch 50/61 - Loss: 0.6157\n",
            "  Batch 60/61 - Loss: 0.3987\n",
            "Loss: 0.4886 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== Fold 4 =====\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "\n",
            "===== EPOCH 1/20 =====\n",
            "  Batch 1/61 - Loss: 0.6921\n",
            "  Batch 10/61 - Loss: 0.6661\n",
            "  Batch 20/61 - Loss: 0.5953\n",
            "  Batch 30/61 - Loss: 0.4974\n",
            "  Batch 40/61 - Loss: 0.5913\n",
            "  Batch 50/61 - Loss: 0.5044\n",
            "  Batch 60/61 - Loss: 0.5910\n",
            "Loss: 0.4664 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 2/20 =====\n",
            "  Batch 1/61 - Loss: 0.5552\n",
            "  Batch 10/61 - Loss: 0.4300\n",
            "  Batch 20/61 - Loss: 0.4976\n",
            "  Batch 30/61 - Loss: 0.4904\n",
            "  Batch 40/61 - Loss: 0.3899\n",
            "  Batch 50/61 - Loss: 0.5606\n",
            "  Batch 60/61 - Loss: 0.5243\n",
            "Loss: 0.4471 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 3/20 =====\n",
            "  Batch 1/61 - Loss: 0.3549\n",
            "  Batch 10/61 - Loss: 0.5044\n",
            "  Batch 20/61 - Loss: 0.4729\n",
            "  Batch 30/61 - Loss: 0.4489\n",
            "  Batch 40/61 - Loss: 0.4212\n",
            "  Batch 50/61 - Loss: 0.5801\n",
            "  Batch 60/61 - Loss: 0.5249\n",
            "Loss: 0.4622 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 4/20 =====\n",
            "  Batch 1/61 - Loss: 0.4548\n",
            "  Batch 10/61 - Loss: 0.2673\n",
            "  Batch 20/61 - Loss: 0.3961\n",
            "  Batch 30/61 - Loss: 0.3488\n",
            "  Batch 40/61 - Loss: 0.5528\n",
            "  Batch 50/61 - Loss: 0.5319\n",
            "  Batch 60/61 - Loss: 0.6812\n",
            "Loss: 0.4479 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 5/20 =====\n",
            "  Batch 1/61 - Loss: 0.5147\n",
            "  Batch 10/61 - Loss: 0.4322\n",
            "  Batch 20/61 - Loss: 0.4138\n",
            "  Batch 30/61 - Loss: 0.4618\n",
            "  Batch 40/61 - Loss: 0.4982\n",
            "  Batch 50/61 - Loss: 0.4042\n",
            "  Batch 60/61 - Loss: 0.4871\n",
            "Loss: 0.4462 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 6/20 =====\n",
            "  Batch 1/61 - Loss: 0.5919\n",
            "  Batch 10/61 - Loss: 0.5011\n",
            "  Batch 20/61 - Loss: 0.4436\n",
            "  Batch 30/61 - Loss: 0.5014\n",
            "  Batch 40/61 - Loss: 0.6418\n",
            "  Batch 50/61 - Loss: 0.4370\n",
            "  Batch 60/61 - Loss: 0.4107\n",
            "Loss: 0.4297 F1-Micro: 0.0678 F1-Macro-Avg 0.0455 Precision: 0.5000 Recall: 0.0364\n",
            "\n",
            "===== EPOCH 7/20 =====\n",
            "  Batch 1/61 - Loss: 0.6186\n",
            "  Batch 10/61 - Loss: 0.4314\n",
            "  Batch 20/61 - Loss: 0.4467\n",
            "  Batch 30/61 - Loss: 0.4147\n",
            "  Batch 40/61 - Loss: 0.3919\n",
            "  Batch 50/61 - Loss: 0.3725\n",
            "  Batch 60/61 - Loss: 0.4623\n",
            "Loss: 0.4049 F1-Micro: 0.0690 F1-Macro-Avg 0.0519 Precision: 0.6667 Recall: 0.0364\n",
            "\n",
            "===== EPOCH 8/20 =====\n",
            "  Batch 1/61 - Loss: 0.5132\n",
            "  Batch 10/61 - Loss: 0.4518\n",
            "  Batch 20/61 - Loss: 0.4181\n",
            "  Batch 30/61 - Loss: 0.2927\n",
            "  Batch 40/61 - Loss: 0.4969\n",
            "  Batch 50/61 - Loss: 0.3550\n",
            "  Batch 60/61 - Loss: 0.4627\n",
            "Loss: 0.3907 F1-Micro: 0.1311 F1-Macro-Avg 0.0909 Precision: 0.6667 Recall: 0.0727\n",
            "\n",
            "===== EPOCH 9/20 =====\n",
            "  Batch 1/61 - Loss: 0.3676\n",
            "  Batch 10/61 - Loss: 0.6794\n",
            "  Batch 20/61 - Loss: 0.2651\n",
            "  Batch 30/61 - Loss: 0.4571\n",
            "  Batch 40/61 - Loss: 0.3749\n",
            "  Batch 50/61 - Loss: 0.2893\n",
            "  Batch 60/61 - Loss: 0.4100\n",
            "Loss: 0.3829 F1-Micro: 0.1967 F1-Macro-Avg 0.1818 Precision: 1.0000 Recall: 0.1091\n",
            "\n",
            "===== EPOCH 10/20 =====\n",
            "  Batch 1/61 - Loss: 0.4784\n",
            "  Batch 10/61 - Loss: 0.2457\n",
            "  Batch 20/61 - Loss: 0.6331\n",
            "  Batch 30/61 - Loss: 0.3890\n",
            "  Batch 40/61 - Loss: 0.2359\n",
            "  Batch 50/61 - Loss: 0.3495\n",
            "  Batch 60/61 - Loss: 0.5174\n",
            "Loss: 0.3821 F1-Micro: 0.3243 F1-Macro-Avg 0.2457 Precision: 0.6316 Recall: 0.2182\n",
            "\n",
            "===== EPOCH 11/20 =====\n",
            "  Batch 1/61 - Loss: 0.4855\n",
            "  Batch 10/61 - Loss: 0.3392\n",
            "  Batch 20/61 - Loss: 0.3459\n",
            "  Batch 30/61 - Loss: 0.3128\n",
            "  Batch 40/61 - Loss: 0.4633\n",
            "  Batch 50/61 - Loss: 0.3697\n",
            "  Batch 60/61 - Loss: 0.5631\n",
            "Loss: 0.3733 F1-Micro: 0.3590 F1-Macro-Avg 0.2709 Precision: 0.6087 Recall: 0.2545\n",
            "\n",
            "===== EPOCH 12/20 =====\n",
            "  Batch 1/61 - Loss: 0.2724\n",
            "  Batch 10/61 - Loss: 0.3071\n",
            "  Batch 20/61 - Loss: 0.2156\n",
            "  Batch 30/61 - Loss: 0.3439\n",
            "  Batch 40/61 - Loss: 0.4282\n",
            "  Batch 50/61 - Loss: 0.3632\n",
            "  Batch 60/61 - Loss: 0.3540\n",
            "Loss: 0.3656 F1-Micro: 0.3636 F1-Macro-Avg 0.3136 Precision: 0.6364 Recall: 0.2545\n",
            "\n",
            "===== EPOCH 13/20 =====\n",
            "  Batch 1/61 - Loss: 0.3655\n",
            "  Batch 10/61 - Loss: 0.2671\n",
            "  Batch 20/61 - Loss: 0.2978\n",
            "  Batch 30/61 - Loss: 0.2120\n",
            "  Batch 40/61 - Loss: 0.4429\n",
            "  Batch 50/61 - Loss: 0.3363\n",
            "  Batch 60/61 - Loss: 0.2691\n",
            "Loss: 0.3538 F1-Micro: 0.3099 F1-Macro-Avg 0.2696 Precision: 0.6875 Recall: 0.2000\n",
            "\n",
            "===== EPOCH 14/20 =====\n",
            "  Batch 1/61 - Loss: 0.4086\n",
            "  Batch 10/61 - Loss: 0.3399\n",
            "  Batch 20/61 - Loss: 0.3610\n",
            "  Batch 30/61 - Loss: 0.2218\n",
            "  Batch 40/61 - Loss: 0.4559\n",
            "  Batch 50/61 - Loss: 0.2678\n",
            "  Batch 60/61 - Loss: 0.3967\n",
            "Loss: 0.3300 F1-Micro: 0.2985 F1-Macro-Avg 0.2820 Precision: 0.8333 Recall: 0.1818\n",
            "\n",
            "===== EPOCH 15/20 =====\n",
            "  Batch 1/61 - Loss: 0.3435\n",
            "  Batch 10/61 - Loss: 0.3471\n",
            "  Batch 20/61 - Loss: 0.2563\n",
            "  Batch 30/61 - Loss: 0.2568\n",
            "  Batch 40/61 - Loss: 0.3026\n",
            "  Batch 50/61 - Loss: 0.1987\n",
            "  Batch 60/61 - Loss: 0.2717\n",
            "Loss: 0.3325 F1-Micro: 0.4416 F1-Macro-Avg 0.4006 Precision: 0.7727 Recall: 0.3091\n",
            "\n",
            "===== EPOCH 16/20 =====\n",
            "  Batch 1/61 - Loss: 0.2762\n",
            "  Batch 10/61 - Loss: 0.3866\n",
            "  Batch 20/61 - Loss: 0.2310\n",
            "  Batch 30/61 - Loss: 0.1794\n",
            "  Batch 40/61 - Loss: 0.1708\n",
            "  Batch 50/61 - Loss: 0.1538\n",
            "  Batch 60/61 - Loss: 0.3033\n",
            "Loss: 0.3249 F1-Micro: 0.4110 F1-Macro-Avg 0.3825 Precision: 0.8333 Recall: 0.2727\n",
            "\n",
            "===== EPOCH 17/20 =====\n",
            "  Batch 1/61 - Loss: 0.2206\n",
            "  Batch 10/61 - Loss: 0.1648\n",
            "  Batch 20/61 - Loss: 0.2352\n",
            "  Batch 30/61 - Loss: 0.1445\n",
            "  Batch 40/61 - Loss: 0.1955\n",
            "  Batch 50/61 - Loss: 0.1554\n",
            "  Batch 60/61 - Loss: 0.3509\n",
            "Loss: 0.3367 F1-Micro: 0.4211 F1-Macro-Avg 0.3684 Precision: 0.7619 Recall: 0.2909\n",
            "\n",
            "===== EPOCH 18/20 =====\n",
            "  Batch 1/61 - Loss: 0.2181\n",
            "  Batch 10/61 - Loss: 0.2722\n",
            "  Batch 20/61 - Loss: 0.2526\n",
            "  Batch 30/61 - Loss: 0.2525\n",
            "  Batch 40/61 - Loss: 0.3077\n",
            "  Batch 50/61 - Loss: 0.2165\n",
            "  Batch 60/61 - Loss: 0.2373\n",
            "Loss: 0.3230 F1-Micro: 0.4054 F1-Macro-Avg 0.3764 Precision: 0.7895 Recall: 0.2727\n",
            "\n",
            "===== EPOCH 19/20 =====\n",
            "  Batch 1/61 - Loss: 0.1404\n",
            "  Batch 10/61 - Loss: 0.2644\n",
            "  Batch 20/61 - Loss: 0.2651\n",
            "  Batch 30/61 - Loss: 0.1724\n",
            "  Batch 40/61 - Loss: 0.1906\n",
            "  Batch 50/61 - Loss: 0.2312\n",
            "  Batch 60/61 - Loss: 0.1864\n",
            "Loss: 0.3032 F1-Micro: 0.5195 F1-Macro-Avg 0.4929 Precision: 0.9091 Recall: 0.3636\n",
            "\n",
            "===== EPOCH 20/20 =====\n",
            "  Batch 1/61 - Loss: 0.1934\n",
            "  Batch 10/61 - Loss: 0.1756\n",
            "  Batch 20/61 - Loss: 0.2190\n",
            "  Batch 30/61 - Loss: 0.2294\n",
            "  Batch 40/61 - Loss: 0.2791\n",
            "  Batch 50/61 - Loss: 0.2602\n",
            "  Batch 60/61 - Loss: 0.1081\n",
            "Loss: 0.3055 F1-Micro: 0.4167 F1-Macro-Avg 0.3929 Precision: 0.8824 Recall: 0.2727\n",
            "\n",
            "===== Fold 5 =====\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "4098\n",
            "\n",
            "===== EPOCH 1/20 =====\n",
            "  Batch 1/61 - Loss: 0.7201\n",
            "  Batch 10/61 - Loss: 0.6283\n",
            "  Batch 20/61 - Loss: 0.5443\n",
            "  Batch 30/61 - Loss: 0.5574\n",
            "  Batch 40/61 - Loss: 0.6208\n",
            "  Batch 50/61 - Loss: 0.5397\n",
            "  Batch 60/61 - Loss: 0.7328\n",
            "Loss: 0.5080 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 2/20 =====\n",
            "  Batch 1/61 - Loss: 0.4373\n",
            "  Batch 10/61 - Loss: 0.5793\n",
            "  Batch 20/61 - Loss: 0.5500\n",
            "  Batch 30/61 - Loss: 0.4518\n",
            "  Batch 40/61 - Loss: 0.6327\n",
            "  Batch 50/61 - Loss: 0.2835\n",
            "  Batch 60/61 - Loss: 0.6242\n",
            "Loss: 0.4679 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 3/20 =====\n",
            "  Batch 1/61 - Loss: 0.3812\n",
            "  Batch 10/61 - Loss: 0.4172\n",
            "  Batch 20/61 - Loss: 0.4641\n",
            "  Batch 30/61 - Loss: 0.5333\n",
            "  Batch 40/61 - Loss: 0.5440\n",
            "  Batch 50/61 - Loss: 0.5511\n",
            "  Batch 60/61 - Loss: 0.6410\n",
            "Loss: 0.4569 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 4/20 =====\n",
            "  Batch 1/61 - Loss: 0.4890\n",
            "  Batch 10/61 - Loss: 0.3898\n",
            "  Batch 20/61 - Loss: 0.6917\n",
            "  Batch 30/61 - Loss: 0.5772\n",
            "  Batch 40/61 - Loss: 0.6096\n",
            "  Batch 50/61 - Loss: 0.4205\n",
            "  Batch 60/61 - Loss: 0.3224\n",
            "Loss: 0.4588 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 5/20 =====\n",
            "  Batch 1/61 - Loss: 0.4254\n",
            "  Batch 10/61 - Loss: 0.6989\n",
            "  Batch 20/61 - Loss: 0.4892\n",
            "  Batch 30/61 - Loss: 0.3654\n",
            "  Batch 40/61 - Loss: 0.5671\n",
            "  Batch 50/61 - Loss: 0.3370\n",
            "  Batch 60/61 - Loss: 0.6564\n",
            "Loss: 0.4746 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 6/20 =====\n",
            "  Batch 1/61 - Loss: 0.3503\n",
            "  Batch 10/61 - Loss: 0.4005\n",
            "  Batch 20/61 - Loss: 0.6663\n",
            "  Batch 30/61 - Loss: 0.4950\n",
            "  Batch 40/61 - Loss: 0.4317\n",
            "  Batch 50/61 - Loss: 0.2870\n",
            "  Batch 60/61 - Loss: 0.4891\n",
            "Loss: 0.4646 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 7/20 =====\n",
            "  Batch 1/61 - Loss: 0.7423\n",
            "  Batch 10/61 - Loss: 0.4943\n",
            "  Batch 20/61 - Loss: 0.4259\n",
            "  Batch 30/61 - Loss: 0.4804\n",
            "  Batch 40/61 - Loss: 0.3826\n",
            "  Batch 50/61 - Loss: 0.3353\n",
            "  Batch 60/61 - Loss: 0.5292\n",
            "Loss: 0.4673 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 8/20 =====\n",
            "  Batch 1/61 - Loss: 0.4860\n",
            "  Batch 10/61 - Loss: 0.3671\n",
            "  Batch 20/61 - Loss: 0.5943\n",
            "  Batch 30/61 - Loss: 0.4956\n",
            "  Batch 40/61 - Loss: 0.6581\n",
            "  Batch 50/61 - Loss: 0.5350\n",
            "  Batch 60/61 - Loss: 0.2957\n",
            "Loss: 0.4598 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 9/20 =====\n",
            "  Batch 1/61 - Loss: 0.5033\n",
            "  Batch 10/61 - Loss: 0.3928\n",
            "  Batch 20/61 - Loss: 0.4686\n",
            "  Batch 30/61 - Loss: 0.4351\n",
            "  Batch 40/61 - Loss: 0.4656\n",
            "  Batch 50/61 - Loss: 0.4950\n",
            "  Batch 60/61 - Loss: 0.5109\n",
            "Loss: 0.4646 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 10/20 =====\n",
            "  Batch 1/61 - Loss: 0.2744\n",
            "  Batch 10/61 - Loss: 0.3892\n",
            "  Batch 20/61 - Loss: 0.4370\n",
            "  Batch 30/61 - Loss: 0.3898\n",
            "  Batch 40/61 - Loss: 0.5527\n",
            "  Batch 50/61 - Loss: 0.5100\n",
            "  Batch 60/61 - Loss: 0.3626\n",
            "Loss: 0.4613 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 11/20 =====\n",
            "  Batch 1/61 - Loss: 0.5631\n",
            "  Batch 10/61 - Loss: 0.5830\n",
            "  Batch 20/61 - Loss: 0.5139\n",
            "  Batch 30/61 - Loss: 0.4058\n",
            "  Batch 40/61 - Loss: 0.3786\n",
            "  Batch 50/61 - Loss: 0.4309\n",
            "  Batch 60/61 - Loss: 0.5370\n",
            "Loss: 0.4613 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 12/20 =====\n",
            "  Batch 1/61 - Loss: 0.6321\n",
            "  Batch 10/61 - Loss: 0.2827\n",
            "  Batch 20/61 - Loss: 0.5456\n",
            "  Batch 30/61 - Loss: 0.5226\n",
            "  Batch 40/61 - Loss: 0.3834\n",
            "  Batch 50/61 - Loss: 0.6568\n",
            "  Batch 60/61 - Loss: 0.3864\n",
            "Loss: 0.4550 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 13/20 =====\n",
            "  Batch 1/61 - Loss: 0.3938\n",
            "  Batch 10/61 - Loss: 0.4434\n",
            "  Batch 20/61 - Loss: 0.5428\n",
            "  Batch 30/61 - Loss: 0.4443\n",
            "  Batch 40/61 - Loss: 0.4226\n",
            "  Batch 50/61 - Loss: 0.4311\n",
            "  Batch 60/61 - Loss: 0.4457\n",
            "Loss: 0.4625 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 14/20 =====\n",
            "  Batch 1/61 - Loss: 0.6057\n",
            "  Batch 10/61 - Loss: 0.7164\n",
            "  Batch 20/61 - Loss: 0.4884\n",
            "  Batch 30/61 - Loss: 0.4827\n",
            "  Batch 40/61 - Loss: 0.5124\n",
            "  Batch 50/61 - Loss: 0.5018\n",
            "  Batch 60/61 - Loss: 0.3521\n",
            "Loss: 0.4632 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 15/20 =====\n",
            "  Batch 1/61 - Loss: 0.5281\n",
            "  Batch 10/61 - Loss: 0.4980\n",
            "  Batch 20/61 - Loss: 0.5395\n",
            "  Batch 30/61 - Loss: 0.6048\n",
            "  Batch 40/61 - Loss: 0.4998\n",
            "  Batch 50/61 - Loss: 0.6104\n",
            "  Batch 60/61 - Loss: 0.6541\n",
            "Loss: 0.4686 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 16/20 =====\n",
            "  Batch 1/61 - Loss: 0.5369\n",
            "  Batch 10/61 - Loss: 0.6098\n",
            "  Batch 20/61 - Loss: 0.4754\n",
            "  Batch 30/61 - Loss: 0.4102\n",
            "  Batch 40/61 - Loss: 0.5742\n",
            "  Batch 50/61 - Loss: 0.5127\n",
            "  Batch 60/61 - Loss: 0.3867\n",
            "Loss: 0.4616 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 17/20 =====\n",
            "  Batch 1/61 - Loss: 0.2883\n",
            "  Batch 10/61 - Loss: 0.5330\n",
            "  Batch 20/61 - Loss: 0.6635\n",
            "  Batch 30/61 - Loss: 0.5448\n",
            "  Batch 40/61 - Loss: 0.5678\n",
            "  Batch 50/61 - Loss: 0.8447\n",
            "  Batch 60/61 - Loss: 0.5009\n",
            "Loss: 0.4574 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 18/20 =====\n",
            "  Batch 1/61 - Loss: 0.6849\n",
            "  Batch 10/61 - Loss: 0.4103\n",
            "  Batch 20/61 - Loss: 0.7662\n",
            "  Batch 30/61 - Loss: 0.3416\n",
            "  Batch 40/61 - Loss: 0.5100\n",
            "  Batch 50/61 - Loss: 0.5248\n",
            "  Batch 60/61 - Loss: 0.6892\n",
            "Loss: 0.4618 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 19/20 =====\n",
            "  Batch 1/61 - Loss: 0.5094\n",
            "  Batch 10/61 - Loss: 0.6745\n",
            "  Batch 20/61 - Loss: 0.4249\n",
            "  Batch 30/61 - Loss: 0.5817\n",
            "  Batch 40/61 - Loss: 0.5436\n",
            "  Batch 50/61 - Loss: 0.3792\n",
            "  Batch 60/61 - Loss: 0.7669\n",
            "Loss: 0.4616 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "\n",
            "===== EPOCH 20/20 =====\n",
            "  Batch 1/61 - Loss: 0.3398\n",
            "  Batch 10/61 - Loss: 0.5457\n",
            "  Batch 20/61 - Loss: 0.4622\n",
            "  Batch 30/61 - Loss: 0.5415\n",
            "  Batch 40/61 - Loss: 0.3769\n",
            "  Batch 50/61 - Loss: 0.3570\n",
            "  Batch 60/61 - Loss: 0.4513\n",
            "Loss: 0.4626 F1-Micro: 0.0000 F1-Macro-Avg 0.0000 Precision: 0.0000 Recall: 0.0000\n",
            "Loss: 0.4196 F1-Micro: 0.2436 F1-Macro-Avg 0.1899 Precision: 0.4649 Recall: 0.1703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "globalMetricsDf, labelMetricsDf = eval_metrics_to_df(evalMetrics)"
      ],
      "metadata": {
        "id": "cx4U3Jhzb7GL"
      },
      "id": "cx4U3Jhzb7GL",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "globalMetricsDf"
      ],
      "metadata": {
        "id": "tEGkj_3lcJJE",
        "outputId": "01ed9d96-57f6-4b8b-8370-498bbe3addbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "id": "tEGkj_3lcJJE",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       loss  accuracy  precision  recall  f1_micro  f1_macro_avg  \\\n",
              "0  0.419626  0.151828   0.464892  0.1703  0.243599      0.189893   \n",
              "\n",
              "   f1_weighted_avg  \n",
              "0          0.19136  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62ec20fc-ea8a-4894-a89b-c4795edc0ae0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_micro</th>\n",
              "      <th>f1_macro_avg</th>\n",
              "      <th>f1_weighted_avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.419626</td>\n",
              "      <td>0.151828</td>\n",
              "      <td>0.464892</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.243599</td>\n",
              "      <td>0.189893</td>\n",
              "      <td>0.19136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62ec20fc-ea8a-4894-a89b-c4795edc0ae0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62ec20fc-ea8a-4894-a89b-c4795edc0ae0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62ec20fc-ea8a-4894-a89b-c4795edc0ae0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_8aa2f14b-cf6c-41d2-9d9e-acb1f5cab2ac\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('globalMetricsDf')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8aa2f14b-cf6c-41d2-9d9e-acb1f5cab2ac button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('globalMetricsDf');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "globalMetricsDf",
              "summary": "{\n  \"name\": \"globalMetricsDf\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.4196260882417361,\n        \"max\": 0.4196260882417361,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4196260882417361\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.1518279569892473,\n        \"max\": 0.1518279569892473,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.1518279569892473\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.46489164086687307,\n        \"max\": 0.46489164086687307,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.46489164086687307\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.17030001507613446,\n        \"max\": 0.17030001507613446,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.17030001507613446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_micro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.24359940872135993,\n        \"max\": 0.24359940872135993,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.24359940872135993\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro_avg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.18989331485378916,\n        \"max\": 0.18989331485378916,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.18989331485378916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_weighted_avg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.19135985230850577,\n        \"max\": 0.19135985230850577,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.19135985230850577\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labelMetricsDf"
      ],
      "metadata": {
        "id": "ScOVPYeScJXh",
        "outputId": "471019ed-c062-4a53-855e-b14db4c5662d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "id": "ScOVPYeScJXh",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      precision    recall  f1-score  support\n",
              "aide_hygiene_personnelle               0.400000  0.188889  0.232143      6.4\n",
              "commission_faire_marche                0.100000  0.057143  0.072727      6.0\n",
              "cuisiner                               0.516667  0.330000  0.363377      6.8\n",
              "logistique_medicale                    0.000000  0.000000  0.000000      5.4\n",
              "menage_epoussetage                     0.400000  0.320000  0.350000      4.6\n",
              "menage_lessive                         0.333333  0.283333  0.304762      5.0\n",
              "menage_plancher                        0.000000  0.000000  0.000000      3.8\n",
              "mobilite_confort                       0.000000  0.000000  0.000000      5.4\n",
              "mobilite_transport_accompagnement_rv   0.400000  0.250000  0.304762      6.2\n",
              "nourrir_personne                       0.303333  0.323810  0.287143      5.6\n",
              "stimulation_dame_compagnie             0.200000  0.153846  0.173913      7.4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc9d412f-a1e1-4cb8-b763-da9cfc83e74b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aide_hygiene_personnelle</th>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.188889</td>\n",
              "      <td>0.232143</td>\n",
              "      <td>6.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>commission_faire_marche</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.072727</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cuisiner</th>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.330000</td>\n",
              "      <td>0.363377</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>logistique_medicale</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>menage_epoussetage</th>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>menage_lessive</th>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.283333</td>\n",
              "      <td>0.304762</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>menage_plancher</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mobilite_confort</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mobilite_transport_accompagnement_rv</th>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.304762</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nourrir_personne</th>\n",
              "      <td>0.303333</td>\n",
              "      <td>0.323810</td>\n",
              "      <td>0.287143</td>\n",
              "      <td>5.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stimulation_dame_compagnie</th>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>7.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc9d412f-a1e1-4cb8-b763-da9cfc83e74b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc9d412f-a1e1-4cb8-b763-da9cfc83e74b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc9d412f-a1e1-4cb8-b763-da9cfc83e74b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9429c680-bc57-4da9-bfb1-fe6bd5dde10a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9429c680-bc57-4da9-bfb1-fe6bd5dde10a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9429c680-bc57-4da9-bfb1-fe6bd5dde10a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e5c563ec-fa94-4c17-b5bb-72cd70a521c0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('labelMetricsDf')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e5c563ec-fa94-4c17-b5bb-72cd70a521c0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('labelMetricsDf');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labelMetricsDf",
              "summary": "{\n  \"name\": \"labelMetricsDf\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18974996839275232,\n        \"min\": 0.0,\n        \"max\": 0.5166666666666667,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.4,\n          0.1,\n          0.30333333333333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13817803421640165,\n        \"min\": 0.0,\n        \"max\": 0.32999999999999996,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.3238095238095238,\n          0.05714285714285714,\n          0.2833333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1-score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14680747792189347,\n        \"min\": 0.0,\n        \"max\": 0.36337662337662335,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07272727272727272,\n          0.30476190476190473,\n          0.23214285714285712\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.021229205863743,\n        \"min\": 3.8,\n        \"max\": 7.4,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          5.6,\n          6.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}